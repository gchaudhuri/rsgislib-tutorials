{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "538af5d1-14c2-42c1-bc7d-970f5a98231f",
   "metadata": {},
   "source": [
    "# Performing a Multivariate Regression\n",
    "\n",
    "The regression analysis we have applied in the other notebooks treats each dependent variable independently, with a different model for each. However, such an approach does not preserve the covariance/correlations between these dependent variables, therefore we should use multivariate regression to predict all three response variables simultaneously. To undertake this analysis, we will use the \"Extra Trees Regressor\" from the scikit-learn library but we could also use other regressors such as:\n",
    "\n",
    " 1. Ordinary Least Squares Linear regressor\n",
    " 2. Partial Least Squares (PLS) regressor\n",
    " 3. Elastic Net regressor\n",
    " 4. Kernel Ridge regressor\n",
    " 5. k-Nearest Neighbours regressor\n",
    " 6. Extra Trees regressor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92161114-0acc-439d-a51a-548646aa31f1",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4470c726-d042-4089-8811-9097c39f3f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import joblib\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import pandas\n",
    "import rsgislib.imageutils\n",
    "import rsgislib.regression\n",
    "import rsgislib.regression.regresssklearn\n",
    "import rsgislib.tools.plotting\n",
    "import rsgislib.tools.utils\n",
    "import scipy.stats\n",
    "import sklearn.model_selection\n",
    "from sklearn.ensemble import ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfca03c-9e9e-4e93-8356-c2ed9253a554",
   "metadata": {},
   "source": [
    "# 2. Read the input plot data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06d5696f-3fe5-4583-99db-513f23630f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the CSV file as a Pandas data frame - the df variable.\n",
    "df = pandas.read_csv(\"../data/lidar/Forest_Plot_Metrics_LassoLars_Sel.csv\", index_col=0)\n",
    "\n",
    "# Get a list of the columns within the df dataframe\n",
    "cols = list(df.columns)\n",
    "\n",
    "# Get the indepedent predictor column names\n",
    "ind_vars = cols[6:]\n",
    "\n",
    "# Get the dependent response column names\n",
    "dep_vars = cols[3:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0e0727-6a27-4fa7-9a80-7d7439cf34d6",
   "metadata": {},
   "source": [
    "# 3. Create output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c6576fa-943b-4b6d-be71-23ccaf00fedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"et_multivar_reg_outputs\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8892fe-6312-415b-8e59-60baae2e6ad5",
   "metadata": {},
   "source": [
    "# 4. Performing a grid-search for optimal hyper-parameters\n",
    "\n",
    "Before we can perform the regression analysis, we must tune our models to ensure that we are using the optimal hyper-parameters. The optimal hyper-parameters are specific to each algorithm and will vary depending on the training data.\n",
    "\n",
    "A grid-search is one popular method of identifying the optimal hyper-parameters. In an exhaustive grid search, a matrix of all possible parameter combinations is generated (e.g. using latin hypercube sampling). Each parameter combination is then tested and accuracy metrics are computed (using cross-validation) to determine how well the model was able to fit the training data. We can identify the best hyper-parameters by finding the parameter combinations that have produced the highest accuracy scores (or lowest errors).\n",
    "\n",
    "A grid search can take a long time (e.g., several hours) when we want to test all possible hyper-parameter combinations on a large training dataset (e.g., > 100,000 samples). To speed up the process, we can randomly sample the parameter grid rather than test all possible parameter combinations. This approach is less accurate than performing an exhaustive search.\n",
    "\n",
    "To find the optimal parameters the following steps needs to be undertaken, example given using the ExtraTreesRegressor algorithm. \n",
    "\n",
    "## 4.1 Read the field plot data in using Pandas as we’ve done in the other scripts\n",
    "\n",
    "*Already done*\n",
    "\n",
    "## 4.2 Split out the dependent and predictor variables from the dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "553a266f-4d05-4293-9748-b85d2fbfc508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictor variables and dependent variables\n",
    "# from the dataframe as numpy arrays\n",
    "x = df[ind_vars].values\n",
    "y = df[dep_vars].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1904990-b739-44b5-8e2c-f8396d1f280f",
   "metadata": {},
   "source": [
    "## 4.3 Get the ExtraTreesRegressor object and parameters to be searched.\n",
    " \n",
    "The function call creates the grid of parameters to be searched, the ExtraTreesRegressor class instance and returns a Boolean as to whether the input data should be scaled or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71d47411-fd14-4b99-ab8c-1f7e99902db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Extra Trees Regressor Object and Grid Search Parameters\n",
    "(\n",
    "    regrs_obj,\n",
    "    regrs_params,\n",
    "    scale_data,\n",
    ") = rsgislib.regression.regresssklearn.get_et_obj_params(len(ind_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e0418d-6618-418d-9276-9a4ad51cb029",
   "metadata": {},
   "source": [
    "## 4.4 Rescale the data if needed. \n",
    "\n",
    "In this case, we are using the scikit-learn StandardScaler (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) which standardizes features by removing the mean and scaling to unit variance. (*Note, for Extra Trees scaling is not needed*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aee18501-b977-447f-bf61-0616610e655c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if scale_data:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    data_scaler = StandardScaler()\n",
    "    data_scaler.fit(x)\n",
    "else:\n",
    "    data_scaler = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b9466a-b680-47d7-9e0b-4bc635d963cb",
   "metadata": {},
   "source": [
    "\n",
    "## 4.5 Get a subset of the data\n",
    "\n",
    "Parameter optimisation is computationally expensive so we cannot use the full datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7b2d852-9ba7-4540-976a-ad2258d0d819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample the input data to create training and testing (20% sample) datasets\n",
    "# so we have an independent dataset to test the quality of the relationship\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "    x, y, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "# Split the training data to have a subset for optimising the parameters.\n",
    "# Using a subset significantly speeds up this analysis.\n",
    "x_data, x_opt_data, y_data, y_opt_data = sklearn.model_selection.train_test_split(\n",
    "    x_train, y_train, test_size=0.3, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9798b993-fa8d-4884-9fa1-6e156d200ead",
   "metadata": {},
   "source": [
    "## 4.6 Create a hyperparameter search object (GridSearchCV or RandomizedSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d9f6b26-8a0b-4fef-8262-55df712a38d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the search (Grid Search or Random Search) object\n",
    "skl_srch_obj = rsgislib.regression.regresssklearn.create_search_obj(\n",
    "    regrs_obj, regrs_params, n_runs=5, n_cv=2, n_cores=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36406b9-fd78-41e7-8639-25454747584d",
   "metadata": {},
   "source": [
    "## 4.7 Perform the parameter search and save those ‘best’ parameters for future use.\n",
    "\n",
    "*Note. this can take quite a long time to run!*\n",
    "\n",
    "If this does take a long time to run, you can skip running the this box as the required file, outputted from the function below, has been provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f89f12c2-5bbe-41f7-9922-cf6f0799c575",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# If you want to run this cell comment out line above.\n",
    "\n",
    "# Perform the parameter search\n",
    "opt_params_file = os.path.join(out_dir, \"regrs_opt_params_et.json\")\n",
    "skl_regrs_opt_obj = rsgislib.regression.regresssklearn.perform_search_param_opt(\n",
    "    opt_params_file,\n",
    "    x_opt_data,\n",
    "    y_opt_data,\n",
    "    skl_srch_obj=skl_srch_obj,\n",
    "    data_scaler=data_scaler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a0c6e2-12a5-4ef7-9e9d-57a5ad8d08c8",
   "metadata": {},
   "source": [
    "# 5. Regression analysis with k-fold cross-validation\n",
    "\n",
    "Having found the optimal hyper-parameters for each machine learning algorithm, we can proceed with the regression analysis. Since we lack independent test data to evaluate each regression model, **resampling methods** must be employed to evaluate the **accuracy** and **precision** of each model:\n",
    "\n",
    "![Regression Fitting Methods](figures/regression_fitting_methods.png)\n",
    "\n",
    "In this tutorial, we will use the train-test split method, with 80% of the forest inventory plots used for training and 20% for validation. Here is a summary of the procedure:\n",
    "\n",
    " * randomly shuffle the data to remove any ordering,\n",
    " * split the data into training and test partitions (80% training, 20% test),\n",
    " * fit the regressor on the training sample and then generate predictions for the test sample,\n",
    " * calculate accuracy statistics to measure the prediction error on the test sample.\n",
    "\n",
    "This description illustrates just one iteration of the train-test split. However, if we want to evaluate the **precision** (i.e., stability) of the regression model, we must perform **several replicates** of the train-test split. By performing several iterations/replicates, **we can evaluate how robust the regression model is to sampling variations**.\n",
    "\n",
    "Note. By performing several iterations/replicates of the cross-validation procedure, we are **attempting to mimic the variability that would exist if several independent forest inventories were collected in our study region** (i.e. we would expect the training data to differ between separate forest inventories due to sampling variations).\n",
    "\n",
    "A model that is overfitted to the training data will have a high bias, whereas a model that is underfitted will have high variance. The optimal model will generate unbiased predictions whilst also being robust to variations in the training data (bias-variance tradeoff).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f94a86b-f2c5-4099-9aa2-317dc3122a4b",
   "metadata": {},
   "source": [
    "## 5.1 Read the Optimised Parameters from file\n",
    "These are the parameters which can be created in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f87e9c1-6429-4947-bba1-4f7d3a52a0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5,\n",
       " 'max_features': 9,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 4,\n",
       " 'n_estimators': 100}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the optimised parameters for the Extra Trees Model\n",
    "opt_params_file = \"../data/lidar/regrs_opt_params_et.json\"\n",
    "skregrs_params = rsgislib.tools.utils.read_json_to_dict(opt_params_file)\n",
    "skregrs_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeffbc00-97aa-41f6-9590-83c060f15992",
   "metadata": {},
   "source": [
    "## 5.2 Create the Instanace with the parameters for the Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d02792fe-10f8-442f-aa9e-1fbd4bfc1837",
   "metadata": {},
   "outputs": [],
   "source": [
    "skregrs_obj = ExtraTreesRegressor(**skregrs_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c2ab8d-d69f-473f-a816-be1881bf7a15",
   "metadata": {},
   "source": [
    "## 5.3 Perform a kfold Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6eda5c-5fb6-4b3e-b3db-0a32b74ca8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 16.91it/s]"
     ]
    }
   ],
   "source": [
    "metrics, residuals = rsgislib.regression.regresssklearn.perform_kfold_fit(\n",
    "    skregrs_obj, x, y, n_splits=5, repeats=20, shuffle=False, data_scaler=data_scaler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedfb987-545c-40fa-975e-97dbbfbfbd48",
   "metadata": {},
   "source": [
    "## 5.4 Export Metrics and Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4610b945-54ae-49d4-8917-1b36836aa024",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dep_var in enumerate(dep_vars):\n",
    "    # Remove spaces (replaced with underscores) and any puntuation from\n",
    "    # the variable name so it can be used within as part of the output\n",
    "    # file name\n",
    "    dep_var_chk = rsgislib.tools.utils.check_str(\n",
    "        dep_var, rm_non_ascii=True, rm_dashs=True, rm_spaces=True, rm_punc=True\n",
    "    ).lower()\n",
    "\n",
    "    df_metrics = pandas.DataFrame(data=metrics[i])\n",
    "    # Save the dataframe to a CSV file.\n",
    "    out_csv_file = os.path.join(\n",
    "        out_dir, \"Forest_Plot_Regres_Metrics_ET_{}.csv\".format(dep_var_chk)\n",
    "    )\n",
    "    df_metrics.to_csv(out_csv_file)\n",
    "\n",
    "    df_residuals = pandas.DataFrame(data=residuals[i])\n",
    "    # Save the dataframe to a CSV file.\n",
    "    out_csv_file = os.path.join(\n",
    "        out_dir, \"Forest_Plot_Regres_Residuals_ET_{}.csv\".format(dep_var_chk)\n",
    "    )\n",
    "    df_residuals.to_csv(out_csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc83cd64-b817-48e0-83a0-53c0d9e4e39e",
   "metadata": {},
   "source": [
    "## 5.5 Summarise Metrics\n",
    "\n",
    "The next step is to summarise metrics we have just outputted from the previous code so we can try and understand the quality of the results and if we had run multiple regression algorithms then which had provided the best results. \n",
    "\n",
    "For this analysis we want to summarise each individual set of outputs for each of the dependent variable. The metrics we will focus on are:\n",
    "\n",
    " 1. The coefficient of determination (r2),\n",
    " 2. Root Mean Square Error (RMSE),\n",
    " 3. Bias.\n",
    "\n",
    "**Note**. *When comparing quoted values to your results or running multiple times, you likely find that your values are slightly different and this is because the plot data is randomly shuffled during the kfold and your shuffles will be different to mine and each time you run the analysis and therefore the results will be slightly different.*\n",
    "\n",
    "\n",
    "As the name implies, the RMSE is the square root of the mean squared errors between our predicted and observed response variables:\n",
    "\n",
    "$\n",
    "\\text{RMSE} = \\sqrt{\\sum_{i=0}^{n}{\\frac{(\\hat{y}_{i} - y_{i})^{2}}{n}}}\n",
    "$\n",
    "\n",
    "RMSE quantifies the quadratic mean error of our regression model in the same units of measurement as our original data. For example, for the Extra Trees regressor, the RMSE for mean DBH was 3.3 cm, whilst the RMSE for basal area was 7.7 m2 ha-1.\n",
    "\n",
    "\n",
    "To compare RMSE values across variables with different measurement units, we can normalise the RMSE into a percentage error by dividing the RMSE by the mean of the response variable and then multiplying by 100:\n",
    "\n",
    "$\n",
    "n\\text{RMSE} (\\%) = 100 \\times \\frac{\\text{RMSE}}{\\bar{y}} \n",
    "$\n",
    "\n",
    "We can now easily compare the RMSE values across all of our response variables, for example for ET:\n",
    " * nRMSE of mean DBH = 19.7%\n",
    " * nRMSE of basal area per hectare = 20.8%\n",
    " * nRMSE of stem volume per hectare = 22.1%.\n",
    "\n",
    "The bias of an estimator is another important accuracy measure. It is simply the mean difference between our predicted and observed response variables:\n",
    "\n",
    "$\n",
    "\\text{Bias} = \\sum_{i=1}^{n}\\frac{\\hat{y}_{i} - y_{i}}{n}\n",
    "$\n",
    "\n",
    "Like RMSE, bias is quantified in the same measurement units as our original data. However, bias can be positive or negative. Negative bias indicates that our model is, on average, underestimating the true values in our response variables. Conversely, positive bias indicates that our model is overestimating the true/observed values in our response variables.\n",
    "\n",
    "An unbiased estimator (bias = zero) is preferable to a biased model (bias not zero), however biased estimators are frequently used when:\n",
    " 1. an unbiased estimator is not available, or\n",
    " 2. a biased estimator gives significantly lower RMSE or higher r2 values.\n",
    "\n",
    "To allow bias values to be compared across variables with different measurement units, we can normalise the absolute bias into a percentage by dividing the bias by the mean of the response variable and then multiplying by 100:\n",
    "\n",
    "$\n",
    "n\\text{Bias} (\\%) = 100 \\times \\frac{Bias}{\\bar{y}} \n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6061c5-5a84-4b06-a528-d2b88bfcd63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dep_var in enumerate(dep_vars):\n",
    "    print(dep_var)\n",
    "    metric_cols = list(metrics[i])\n",
    "    for metric in metric_cols:\n",
    "        print(\n",
    "            \"\\t{}: Min: {}, Mean: {}, Median: {}, Max:{}, Std Dev: {}\".format(\n",
    "                metric,\n",
    "                numpy.min(metrics[i][metric]),\n",
    "                numpy.mean(metrics[i][metric]),\n",
    "                numpy.median(metrics[i][metric]),\n",
    "                numpy.max(metrics[i][metric]),\n",
    "                numpy.std(metrics[i][metric]),\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8238d9bc-5838-43cf-8bb6-f00e7e493369",
   "metadata": {},
   "source": [
    "## 5.6. Plot Residuals\n",
    "\n",
    "A residual plot is a useful method of visually investigating the distribution of errors in our regression model. We can use the residual plot to investigate both the normality and homoscedasticity of the model errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a987d5-ffd7-460c-9fa2-4f272ef9f867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get residuals as dataframes for each dependent variable\n",
    "for i, dep_var in enumerate(dep_vars):\n",
    "    if dep_var == \"BA / ha\":\n",
    "        df_ba_res = pandas.DataFrame(data=residuals[i])\n",
    "    elif dep_var == \"Vol / ha\":\n",
    "        df_vol_res = pandas.DataFrame(data=residuals[i])\n",
    "    elif dep_var == \"Mean DBH\":\n",
    "        df_dbh_res = pandas.DataFrame(data=residuals[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5472d9a8-64f4-4992-8b7c-685d5ea7629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ba_y_true = df_ba_res[\"y_true\"].values\n",
    "ba_residuals = df_ba_res[\"y_pred\"].values - ba_y_true\n",
    "\n",
    "ba_resid_plt_file = os.path.join(out_dir, \"et_residuals_plot_ba.png\")\n",
    "rsgislib.tools.plotting.residual_plot(\n",
    "    ba_y_true,\n",
    "    ba_residuals,\n",
    "    ba_resid_plt_file,\n",
    "    title=\"Residuals for Basal Area ET Regression\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189e20c1-1db5-494a-99da-24b86d051e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbh_y_true = df_dbh_res[\"y_true\"].values\n",
    "dbh_residuals = df_dbh_res[\"y_pred\"].values - dbh_y_true\n",
    "\n",
    "dbh_resid_plt_file = os.path.join(out_dir, \"et_residuals_plot_dbh.png\")\n",
    "rsgislib.tools.plotting.residual_plot(\n",
    "    dbh_y_true,\n",
    "    dbh_residuals,\n",
    "    dbh_resid_plt_file,\n",
    "    title=\"Residuals for DBH ET Regression\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f28b4a5-14ee-4387-ae6f-6bd71078b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_y_true = df_vol_res[\"y_true\"].values\n",
    "vol_residuals = df_vol_res[\"y_pred\"].values - vol_y_true\n",
    "\n",
    "vol_resid_plt_file = os.path.join(out_dir, \"et_residuals_plot_vol.png\")\n",
    "rsgislib.tools.plotting.residual_plot(\n",
    "    vol_y_true,\n",
    "    vol_residuals,\n",
    "    vol_resid_plt_file,\n",
    "    title=\"Residuals for Stem Volume ET Regression\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da1c85f-c8a9-4b37-afb6-2a81bf295b6e",
   "metadata": {},
   "source": [
    "![BA Residuals](figures/et_residuals_plot_ba.png)\n",
    "![DbH Residuals](figures/et_residuals_plot_dbh.png)\n",
    "![Volume Residuals](figures/et_residuals_plot_vol.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4976abe2-8958-42ba-8e11-f20c4f70a086",
   "metadata": {},
   "source": [
    "From these plots we can see that there is a clear trend in our residuals which indicates that our ET model is underestimating plots with larger values. For example, for plots with a stem volume > 600 m2 ha-1 there is a clear underestimation. If we were to investigate this further, it is likely that we would find that there are a small number of plots with high stem volume and therefore the training data has not well sampled these plots so the model is not well fitted to these plots. If it would possible, we could look to increase the number of these plot without are training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5eba4f-fffb-4ed1-8d7a-49eb20d1bb75",
   "metadata": {},
   "source": [
    "## 5.7 Investigating the normality of residuals\n",
    "\n",
    "A quantile-quantile plot is a useful method of visually investigating whether the errors/residuals from our model follow a normal (Gaussian) distribution.\n",
    "\n",
    "In python, a Q-Q plot can be produced using the scipy.stats.probplot() function. The code has been prepared for you to generate a Q-Q plot showing the normality of residuals for basal area, mean DBH and stem volume per hectare for the linear model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597a7bce-d527-4480-9c5f-7ba3491b07be",
   "metadata": {},
   "outputs": [],
   "source": [
    "ba_resid_qq_plt_file = os.path.join(out_dir, \"et_residuals_qq_plot_ba.png\")\n",
    "rsgislib.tools.plotting.quantile_plot(\n",
    "    ba_residuals,\n",
    "    \"Basal Area error (m$^2$ ha$^{-1}$)\",\n",
    "    ba_resid_qq_plt_file,\n",
    "    title=\"Residuals for Basal Area OSL Regression\",\n",
    ")\n",
    "\n",
    "dbh_resid_qq_plt_file = os.path.join(out_dir, \"et_residuals_qq_plot_dbh.png\")\n",
    "rsgislib.tools.plotting.quantile_plot(\n",
    "    dbh_residuals,\n",
    "    \"DBH error (cm)\",\n",
    "    dbh_resid_qq_plt_file,\n",
    "    title=\"Residuals for DBH OSL Regression\",\n",
    ")\n",
    "\n",
    "vol_resid_qq_plt_file = os.path.join(out_dir, \"et_residuals_qq_plot_vol.png\")\n",
    "rsgislib.tools.plotting.quantile_plot(\n",
    "    vol_residuals,\n",
    "    \"Volume error (m$^3$ ha$^{-1}$)\",\n",
    "    vol_resid_qq_plt_file,\n",
    "    title=\"Residuals for Stem Volume OSL Regression\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbcac38-169f-47a9-ac62-317747d7c5db",
   "metadata": {},
   "source": [
    "![BA QQ Residuals](figures/et_residuals_qq_plot_ba.png)\n",
    "![DbH QQ Residuals](figures/et_residuals_qq_plot_dbh.png)\n",
    "![Volume QQ Residuals](figures/et_residuals_qq_plot_vol.png)\n",
    "\n",
    "The Q-Q plots demonstrates that our residuals deviate from a normal distribution (represented by the red line). This is especially noticeable for plots with low stem volumes and low DBH values.\n",
    "\n",
    "To test the significance of these deviations from normality, we can use several functions available in SciPy:\n",
    " * Shapiro-Wilk Test - scipy.stats.shapiro()\n",
    " * Kolmogorov-Smirnov Test - scipy.stats.kstest()\n",
    " * Anderson-Darling Test - scipy.stats.anderson()\n",
    "\n",
    "We will use the Shapiro-Wilk Test, where the null hypothesis for this test is that the sample is normally distributed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01b12e1-d1bc-44d4-8227-f579e19b35c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pval, w = scipy.stats.shapiro(ba_residuals)\n",
    "print(\"ba: p = {}\".format(round(pval, 4)))\n",
    "if pval > 0.05:\n",
    "    print(\n",
    "        \"Accept the null hypothesis that the residuals are Gaussian at the 95% confidence interval (BA)\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"Reject the null hypothesis that the residuals are Gaussian at the 95% confidence interval (BA)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f71b31-6238-47a6-8574-efce8c8390d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pval, w = scipy.stats.shapiro(dbh_residuals)\n",
    "print(\"dbh: p = {}\".format(round(pval, 4)))\n",
    "if pval > 0.05:\n",
    "    print(\n",
    "        \"Accept the null hypothesis that the residuals are Gaussian at the 95% confidence interval (dbh)\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"Reject the null hypothesis that the residuals are Gaussian at the 95% confidence interval (dbh)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1ea1a6-500c-47c9-8fab-bd4c22132fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pval, w = scipy.stats.shapiro(vol_residuals)\n",
    "print(\"vol: p = {}\".format(round(pval, 4)))\n",
    "if pval > 0.05:\n",
    "    print(\n",
    "        \"Accept the null hypothesis that the residuals are Gaussian at the 95% confidence interval (vol)\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"Reject the null hypothesis that the residuals are Gaussian at the 95% confidence interval (vol)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a671a7a-a5b2-4d91-a8ad-dcecf40c7d28",
   "metadata": {},
   "source": [
    "Our p-value is greater than 0.05, therefore we must accept the null hypothesis that the residuals are Gaussian at the 95% confidence interval. If the p-value was < 0.05, we would accept the alternative hypothesis that the residuals are non-Gaussian at the 95% confidence interval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eac584-cad3-4f8a-94c2-abc0d38dab41",
   "metadata": {},
   "source": [
    "# 6. Creating the Final Model\n",
    "\n",
    "We are now ready to apply our model to the image data, the first step is to build the final model and save it for future analysis. To save a scikit-learn model we commonly use the joblib module as recommended by the scikit-learn documentation: https://scikit-learn.org/stable/modules/model_persistence.html. It should be noted that when you’ve saved a model with the joblib module, that model might not be able to be loaded with other versions of scikit-learn depending on what changes occur within the scikit-learn library. You also need to be using the same version of Python, for example if you save the model with Python 3.7 and then try and open it with 3.8 or 3.9 it most likely will not work!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8439c26-2228-4339-9382-28c830a13a62",
   "metadata": {},
   "source": [
    "## 6.1 Read the Optimised Parameters File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b9b9cb-5c5e-403b-b426-85c6bd538978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the optimised parameters for the Extra Trees Model\n",
    "opt_params_file = \"../data/lidar/regrs_opt_params_et.json\"\n",
    "skregrs_params = rsgislib.tools.utils.read_json_to_dict(opt_params_file)\n",
    "skregrs_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6c3be9-8e54-4524-94a7-648bb5589756",
   "metadata": {},
   "source": [
    "## 6.2 Create the Instanace with the parameters for the Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e3a5fe-3f7e-4502-9b80-d0de7e22f9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "skregrs_obj = ExtraTreesRegressor(**skregrs_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c430eb78-e9f7-48bb-8218-fbf67f1dad0a",
   "metadata": {},
   "source": [
    "## 6.3 Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b4eae5-4824-452f-9869-5d64b7cd946c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note x and y were defined in section 4.2 :)\n",
    "\n",
    "skregrs_obj.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4220178d-6878-4a5e-b200-46739bfb441f",
   "metadata": {},
   "source": [
    "## 6.3 Save Model to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af10721-685d-4367-93d8-c8554a0689ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output model file\n",
    "output_mdl_file = os.path.join(out_dir, \"Forest_Reg_ET_Model.joblib\")\n",
    "\n",
    "# Save regression model to disk:\n",
    "print(\"Saving regression model...\")\n",
    "joblib.dump(skregrs_obj, output_mdl_file, (\"gzip\", 1))\n",
    "print(\"Saved regression model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0826c831-1dc4-4813-be1e-94e9960e16c9",
   "metadata": {},
   "source": [
    "## 6.4 Check that the Saved Model loads...\n",
    "\n",
    "You probably don't need this step but it can be useful to double check if it is a file you intend to reuse and come back to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3ec1a1-b94e-4321-a1ec-6180c39b0bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the model can be read from disk:\n",
    "print(\"Attempting to read regression model from disk...\")\n",
    "regrs_test_mdl = joblib.load(output_mdl_file)\n",
    "regrs_test_mdl.predict(x)\n",
    "print(\"Read model and used it predict successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eb67dc-05b4-4359-804f-619cc8f84706",
   "metadata": {},
   "source": [
    "# 7. Apply Model to Image Data\n",
    "\n",
    "To apply the model, we cannot use band maths as we did previously as the model is providing multiple output (i.e., 3). Therefore, we need to use the scikit-learn model directly. To do this we will use the apply_regress_sklearn_mdl function which is within RSGISLib.\n",
    "\n",
    "To ensure we are only applying the regression model to valid regions (i.e., pixel which are forest!) we provide a valid data mask (Forest_ALS_Valid.kea) alongside the LiDAR metrics image (Forest_ALS_Metrics.kea). We need to provide a list of image band numbers to the apply_regress_sklearn_mdl function as we are just using the 12 metrics identified using the LassoLars feature selection method - **Note, these need to be the same and in the same order as used to train the regressor**. The image band numbering starts at 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e46893e-6552-4310-bfd7-e55718e96572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Input Images\n",
    "input_img = \"../data/lidar/Forest_ALS_Metrics.kea\"\n",
    "vld_msk_img = \"../data/lidar/Forest_ALS_Valid.kea\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0164d33-bb30-4af9-8759-6490d738f7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image bands in the metrics image used by the model.\n",
    "metrics_band_idxs = [24, 27, 41, 42, 43, 46, 47, 48, 49, 51, 52, 61]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599c37af-132c-4347-9dcb-687475ed2c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output options for GTIFF.\n",
    "rsgislib.imageutils.set_env_vars_lzw_gtiff_outs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31707bd-f6b7-4d08-bf3b-07df391b9819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Image\n",
    "output_img = os.path.join(out_dir, \"ALS_Forest_Estimates.tif\")\n",
    "out_band_names = [\"Basal Area\", \"Mean DBH\", \"Stem Volume\"]\n",
    "\n",
    "# Apply the model to the image data\n",
    "rsgislib.regression.regresssklearn.apply_regress_sklearn_mdl(\n",
    "    skregrs_obj,\n",
    "    3,\n",
    "    input_img,\n",
    "    metrics_band_idxs,\n",
    "    vld_msk_img,\n",
    "    1,\n",
    "    output_img,\n",
    "    gdalformat=\"GTIFF\",\n",
    "    out_band_names=out_band_names,\n",
    "    calc_stats=True,\n",
    "    out_no_date_val=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4cdaa5-2862-4b33-b85a-99501691dfba",
   "metadata": {},
   "source": [
    "# 8. Visualise Results\n",
    "\n",
    "You should now have the image ALS_Forest_Estimates.tif created and you can open it in TuiView or QGIS. However, we can also makes some plots using Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70796118-30e8-41e2-8505-dd3603ba18db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_bbox = [291171, 301190, 246680, 257452]\n",
    "\n",
    "# Get the image data - vol_out_img\n",
    "img_vol_data, img_vol_coords = rsgislib.tools.plotting.get_gdal_raster_mpl_imshow(\n",
    "    output_img, bands=[3], bbox=sub_bbox\n",
    ")\n",
    "\n",
    "# Get the image data - ba_out_img\n",
    "img_ba_data, img_ba_coords = rsgislib.tools.plotting.get_gdal_raster_mpl_imshow(\n",
    "    output_img, bands=[1], bbox=sub_bbox\n",
    ")\n",
    "\n",
    "# Get the image data - dbh_out_img\n",
    "img_dbh_data, img_dbh_coords = rsgislib.tools.plotting.get_gdal_raster_mpl_imshow(\n",
    "    output_img, bands=[2], bbox=sub_bbox\n",
    ")\n",
    "\n",
    "\n",
    "# Define the colour map\n",
    "# Options: https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "# Edit output bounds values to be white.\n",
    "cmap = plt.get_cmap(\"Greens\")\n",
    "mcolors.Colormap.set_under(cmap, color=(1, 1, 1, 0))\n",
    "mcolors.Colormap.set_over(cmap, color=(1, 1, 1, 0))\n",
    "\n",
    "# Create the matplotlib figure - in this case two plots so the NDVI\n",
    "# and image are plotted alongside each other\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 10), sharey=True)\n",
    "\n",
    "vol_plt = ax1.imshow(\n",
    "    img_vol_data,\n",
    "    extent=img_vol_coords,\n",
    "    cmap=cmap,\n",
    "    norm=mcolors.Normalize(vmin=1, vmax=numpy.max(img_vol_data)),\n",
    ")\n",
    "ax1.title.set_text(\"Vol / Ha\")\n",
    "fig.colorbar(vol_plt, ax=ax1, orientation=\"horizontal\", shrink=0.9)\n",
    "\n",
    "ba_plt = ax2.imshow(\n",
    "    img_ba_data,\n",
    "    extent=img_ba_coords,\n",
    "    cmap=cmap,\n",
    "    norm=mcolors.Normalize(vmin=1, vmax=numpy.max(img_ba_data)),\n",
    ")\n",
    "ax2.title.set_text(\"Ba / Ha\")\n",
    "fig.colorbar(ba_plt, ax=ax2, orientation=\"horizontal\", shrink=0.9)\n",
    "\n",
    "dbh_plt = ax3.imshow(\n",
    "    img_dbh_data,\n",
    "    extent=img_dbh_coords,\n",
    "    cmap=cmap,\n",
    "    norm=mcolors.Normalize(vmin=1, vmax=numpy.max(img_dbh_data)),\n",
    ")\n",
    "ax3.title.set_text(\"DbH\")\n",
    "fig.colorbar(dbh_plt, ax=ax3, orientation=\"horizontal\", shrink=0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
